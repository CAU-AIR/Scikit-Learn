{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia principal eigenvector\n",
    "- 그래프의 정점들의 상대적 중요성을 확인하는 방법은 인접 행렬의 주요 고유벡터를 계산하여 정점에 첫 번쨰 고유벡터의 성분 값을 중심성 점수로 할당하는 것\n",
    "- 위키피디아 문서 내부의 링크 그래프를 분석하여 고유벡터 중심성에 따라 기사를 상대적인 중요도에 따라 순위 매기는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bz2 import BZ2File\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.decomposition import randomized_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data, if not already on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from 'http://downloads.dbpedia.org/3.5.1/en/redirects_en.nt.bz2', please wait...\n",
      "\n",
      "Downloading data from 'http://downloads.dbpedia.org/3.5.1/en/page_links_en.nt.bz2', please wait...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "redirects_url = \"http://downloads.dbpedia.org/3.5.1/en/redirects_en.nt.bz2\"\n",
    "redirects_filename = redirects_url.rsplit(\"/\", 1)[1]\n",
    "\n",
    "page_links_url = \"http://downloads.dbpedia.org/3.5.1/en/page_links_en.nt.bz2\"\n",
    "page_links_filename = page_links_url.rsplit(\"/\", 1)[1]\n",
    "\n",
    "resources = [\n",
    "    (redirects_url, redirects_filename),\n",
    "    (page_links_url, page_links_filename),\n",
    "]\n",
    "\n",
    "for url, filename in resources:\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"Downloading data from '%s', please wait...\" % url)\n",
    "        opener = urlopen(url)\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(opener.read())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the redirect files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index(redirects, index_map, k):\n",
    "    \"\"\"Find the index of an article name after redirect resolution\"\"\"\n",
    "    k = redirects.get(k, k)\n",
    "    return index_map.setdefault(k, len(index_map))\n",
    "\n",
    "\n",
    "DBPEDIA_RESOURCE_PREFIX_LEN = len(\"http://dbpedia.org/resource/\")\n",
    "SHORTNAME_SLICE = slice(DBPEDIA_RESOURCE_PREFIX_LEN + 1, -1)\n",
    "\n",
    "\n",
    "def short_name(nt_uri):\n",
    "    \"\"\"< 와 > URI 마커 및 공통 URI 접두사를 제거\"\"\"\n",
    "    return nt_uri[SHORTNAME_SLICE]\n",
    "\n",
    "\n",
    "def get_redirects(redirects_filename):\n",
    "    \"\"\"리다이렉션을 구문 분ㅅ거하고 그로부터 추이적으로 닫힌 맵을 구축\"\"\"\n",
    "    redirects = {}\n",
    "    print(\"Parsing the NT redirect file\")\n",
    "    for l, line in enumerate(BZ2File(redirects_filename)):\n",
    "        split = line.split()\n",
    "        if len(split) != 4:\n",
    "            print(\"ignoring malformed line: \" + line)\n",
    "            continue\n",
    "        redirects[short_name(split[0])] = short_name(split[2])\n",
    "        if l % 1000000 == 0:\n",
    "            print(\"[%s] line: %08d\" % (datetime.now().isoformat(), l))\n",
    "\n",
    "    # 추이적 종결을 계산\n",
    "    print(\"Computing the transitive closure of the redirect relation\")\n",
    "    for l, source in enumerate(redirects.keys()):\n",
    "        transitive_target = None\n",
    "        target = redirects[source]\n",
    "        seen = {source}\n",
    "        while True:\n",
    "            transitive_target = target\n",
    "            target = redirects.get(target)\n",
    "            if target is None or target in seen:\n",
    "                break\n",
    "            seen.add(target)\n",
    "        redirects[source] = transitive_target\n",
    "        if l % 1000000 == 0:\n",
    "            print(\"[%s] line: %08d\" % (datetime.now().isoformat(), l))\n",
    "\n",
    "    return redirects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the redirect map\n",
      "Parsing the NT redirect file\n",
      "[2024-01-15T14:06:24.989011] line: 00000000\n",
      "[2024-01-15T14:06:27.514060] line: 01000000\n",
      "[2024-01-15T14:06:30.159590] line: 02000000\n",
      "[2024-01-15T14:06:32.860697] line: 03000000\n",
      "[2024-01-15T14:06:35.407544] line: 04000000\n",
      "Computing the transitive closure of the redirect relation\n",
      "[2024-01-15T14:06:35.611357] line: 00000000\n",
      "[2024-01-15T14:06:35.948119] line: 01000000\n",
      "[2024-01-15T14:06:36.307093] line: 02000000\n",
      "[2024-01-15T14:06:36.691475] line: 03000000\n",
      "[2024-01-15T14:06:37.125034] line: 04000000\n",
      "Computing the integer index map\n",
      "[2024-01-15T14:06:37.164342] line: 00000000\n",
      "[2024-01-15T14:06:39.612193] line: 01000000\n",
      "[2024-01-15T14:06:42.146671] line: 02000000\n",
      "[2024-01-15T14:06:44.651617] line: 03000000\n",
      "[2024-01-15T14:06:47.193878] line: 04000000\n",
      "Computing the adjacency matrix\n",
      "Converting to CSR representation\n",
      "CSR conversion done\n"
     ]
    }
   ],
   "source": [
    "def get_adjacency_matrix(redirects_filename, page_links_filename, limit=None):\n",
    "    \"\"\"리다이렉션을 해결한 후, scipy 희소 행렬로서 인접 그래프를 추출\n",
    "\n",
    "    X는 scipy의 희소 행렬이며, 리다이렉트는 아티클 이름에서 아티클 이름으로 이루어진 파이썬 딕셔너리로 반환\n",
    "    index_map은 아티클 이름에서 파이썬 int(아티클 인덱스)로 이루어진 파이썬 딕셔너리이다.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Computing the redirect map\")\n",
    "    redirects = get_redirects(redirects_filename)\n",
    "\n",
    "    print(\"Computing the integer index map\")\n",
    "    index_map = dict()\n",
    "    links = list()\n",
    "    for l, line in enumerate(BZ2File(page_links_filename)):\n",
    "        split = line.split()\n",
    "        if len(split) != 4:\n",
    "            print(\"ignoring malformed line: \" + line)\n",
    "            continue\n",
    "        i = index(redirects, index_map, short_name(split[0]))\n",
    "        j = index(redirects, index_map, short_name(split[2]))\n",
    "        links.append((i, j))\n",
    "        if l % 1000000 == 0:\n",
    "            print(\"[%s] line: %08d\" % (datetime.now().isoformat(), l))\n",
    "\n",
    "        if limit is not None and l >= limit - 1:\n",
    "            break\n",
    "\n",
    "    print(\"Computing the adjacency matrix\")\n",
    "    X = sparse.lil_matrix((len(index_map), len(index_map)), dtype=np.float32)\n",
    "    for i, j in links:\n",
    "        X[i, j] = 1.0\n",
    "    del links\n",
    "    print(\"Converting to CSR representation\")\n",
    "    X = X.tocsr()\n",
    "    print(\"CSR conversion done\")\n",
    "    return X, redirects, index_map\n",
    "\n",
    "\n",
    "# RAM에서 작업이 가능하도록 5백만 개의 링크까지만 처리\n",
    "X, redirects, index_map = get_adjacency_matrix(\n",
    "    redirects_filename, page_links_filename, limit=5000000\n",
    ")\n",
    "names = {i: name for name, i in index_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Principal Singular Vector using Randomized SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the principal singular vectors using randomized_svd\n",
      "done in 1.711s\n",
      "Top wikipedia pages according to principal singular vectors\n",
      "[b'1989',\n",
      " b'1971',\n",
      " b'1975',\n",
      " b'1970',\n",
      " b'2006',\n",
      " b'1972',\n",
      " b'1996',\n",
      " b'1966',\n",
      " b'1967',\n",
      " b'2007']\n",
      "[b'Soviet_Union',\n",
      " b'Spain',\n",
      " b'Italy',\n",
      " b'Canada',\n",
      " b'Japan',\n",
      " b'Germany',\n",
      " b'World_War_II',\n",
      " b'France',\n",
      " b'United_Kingdom',\n",
      " b'United_States']\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing the principal singular vectors using randomized_svd\")\n",
    "t0 = time()\n",
    "U, s, V = randomized_svd(X, 5, n_iter=3)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "# 주요 특이벡터의 관련된 가장 강한 연결 요소의 이름을 출력하고 고유벡터와 유사하게 할 수 있다.\n",
    "# 위키피디아에서 관련된 가장 강한 연결 요소의 이름을 출력\n",
    "print(\"Top wikipedia pages according to principal singular vectors\")\n",
    "pprint([names[i] for i in np.abs(U.T[0]).argsort()[-10:]])\n",
    "pprint([names[i] for i in np.abs(V[0]).argsort()[-10:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Cnetrality scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing principal eigenvector score using a power iteration method\n",
      "Normalizing the graph\n",
      "power iteration #0\n",
      "error: 0.988448\n",
      "power iteration #1\n",
      "error: 0.495766\n",
      "power iteration #2\n",
      "error: 0.304177\n",
      "power iteration #3\n",
      "error: 0.206611\n",
      "power iteration #4\n",
      "error: 0.149243\n",
      "power iteration #5\n",
      "error: 0.112217\n",
      "power iteration #6\n",
      "error: 0.086737\n",
      "power iteration #7\n",
      "error: 0.068371\n",
      "power iteration #8\n",
      "error: 0.054681\n",
      "power iteration #9\n",
      "error: 0.044212\n",
      "power iteration #10\n",
      "error: 0.036058\n",
      "power iteration #11\n",
      "error: 0.029603\n",
      "power iteration #12\n",
      "error: 0.024433\n",
      "power iteration #13\n",
      "error: 0.020259\n",
      "power iteration #14\n",
      "error: 0.016847\n",
      "power iteration #15\n",
      "error: 0.014051\n",
      "power iteration #16\n",
      "error: 0.011746\n",
      "power iteration #17\n",
      "error: 0.009838\n",
      "power iteration #18\n",
      "error: 0.008256\n",
      "power iteration #19\n",
      "error: 0.006936\n",
      "power iteration #20\n",
      "error: 0.005830\n",
      "power iteration #21\n",
      "error: 0.004912\n",
      "power iteration #22\n",
      "error: 0.004139\n",
      "power iteration #23\n",
      "error: 0.003489\n",
      "power iteration #24\n",
      "error: 0.002945\n",
      "power iteration #25\n",
      "error: 0.002485\n",
      "power iteration #26\n",
      "error: 0.002097\n",
      "power iteration #27\n",
      "error: 0.001770\n",
      "power iteration #28\n",
      "error: 0.001495\n",
      "power iteration #29\n",
      "error: 0.001263\n",
      "power iteration #30\n",
      "error: 0.001067\n",
      "power iteration #31\n",
      "error: 0.000902\n",
      "power iteration #32\n",
      "error: 0.000763\n",
      "power iteration #33\n",
      "error: 0.000644\n",
      "power iteration #34\n",
      "error: 0.000545\n",
      "power iteration #35\n",
      "error: 0.000461\n",
      "power iteration #36\n",
      "error: 0.000390\n",
      "power iteration #37\n",
      "error: 0.000330\n",
      "power iteration #38\n",
      "error: 0.000280\n",
      "power iteration #39\n",
      "error: 0.000237\n",
      "power iteration #40\n",
      "error: 0.000200\n",
      "power iteration #41\n",
      "error: 0.000169\n",
      "power iteration #42\n",
      "error: 0.000142\n",
      "power iteration #43\n",
      "error: 0.000121\n",
      "power iteration #44\n",
      "error: 0.000102\n",
      "done in 0.674s\n",
      "[b'Africa',\n",
      " b'Animal',\n",
      " b'New_York',\n",
      " b'Jews',\n",
      " b'Philosophy',\n",
      " b'Byzantine_Empire',\n",
      " b'New_York_City',\n",
      " b'World_War_I',\n",
      " b'France',\n",
      " b'United_States']\n"
     ]
    }
   ],
   "source": [
    "def centrality_scores(X, alpha=0.85, max_iter=100, tol=1e-10):\n",
    "    \"\"\"Power iteration computation of the principal eigenvector\n",
    "\n",
    "    This method is also known as Google PageRank and the implementation\n",
    "    is based on the one from the NetworkX project (BSD licensed too)\n",
    "    with copyrights by:\n",
    "\n",
    "      Aric Hagberg <hagberg@lanl.gov>\n",
    "      Dan Schult <dschult@colgate.edu>\n",
    "      Pieter Swart <swart@lanl.gov>\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    X = X.copy()\n",
    "    incoming_counts = np.asarray(X.sum(axis=1)).ravel()\n",
    "\n",
    "    print(\"Normalizing the graph\")\n",
    "    for i in incoming_counts.nonzero()[0]:\n",
    "        X.data[X.indptr[i] : X.indptr[i + 1]] *= 1.0 / incoming_counts[i]\n",
    "    dangle = np.asarray(np.where(np.isclose(X.sum(axis=1), 0), 1.0 / n, 0)).ravel()\n",
    "\n",
    "    scores = np.full(n, 1.0 / n, dtype=np.float32)  # 추정 시작\n",
    "    for i in range(max_iter):\n",
    "        print(\"power iteration #%d\" % i)\n",
    "        prev_scores = scores\n",
    "        scores = (\n",
    "            alpha * (scores * X + np.dot(dangle, prev_scores))\n",
    "            + (1 - alpha) * prev_scores.sum() / n\n",
    "        )\n",
    "        # 수렴하는 지 체크: 정규화된 l_inf norm\n",
    "        scores_max = np.abs(scores).max()\n",
    "        if scores_max == 0.0:\n",
    "            scores_max = 1.0\n",
    "        err = np.abs(scores - prev_scores).max() / scores_max\n",
    "        print(\"error: %0.6f\" % err)\n",
    "        if err < n * tol:\n",
    "            return scores\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "print(\"Computing principal eigenvector score using a power iteration method\")\n",
    "t0 = time()\n",
    "scores = centrality_scores(X, max_iter=100)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "pprint([names[i] for i in np.abs(scores).argsort()[-10:]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
