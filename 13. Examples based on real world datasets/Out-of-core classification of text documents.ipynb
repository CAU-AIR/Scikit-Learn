{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "import sys\n",
    "import tarfile\n",
    "import time\n",
    "from hashlib import sha256\n",
    "from html.parser import HTMLParser\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import rcParams\n",
    "\n",
    "from sklearn.datasets import get_data_home\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier, Perceptron, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "def _not_in_sphinx():\n",
    "    return \"__file__\" in globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuters 데이터셋을 파싱하는 클래스\n",
    "class ReutersParser(HTMLParser):\n",
    "\n",
    "    def __init__(self, encoding=\"latin-1\"):\n",
    "        HTMLParser.__init__(self)\n",
    "        self._reset()\n",
    "        self.encoding = encoding\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        method = \"start_\" + tag\n",
    "        getattr(self, method, lambda x: None)(attrs)\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        method = \"end_\" + tag\n",
    "        getattr(self, method, lambda: None)()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.in_title = 0\n",
    "        self.in_body = 0\n",
    "        self.in_topics = 0\n",
    "        self.in_topic_d = 0\n",
    "        self.title = \"\"\n",
    "        self.body = \"\"\n",
    "        self.topics = []\n",
    "        self.topic_d = \"\"\n",
    "\n",
    "    def parse(self, fd):\n",
    "        self.docs = []\n",
    "        for chunk in fd:\n",
    "            self.feed(chunk.decode(self.encoding))\n",
    "            for doc in self.docs:\n",
    "                yield doc\n",
    "            self.docs = []\n",
    "        self.close()\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        if self.in_body:\n",
    "            self.body += data\n",
    "        elif self.in_title:\n",
    "            self.title += data\n",
    "        elif self.in_topic_d:\n",
    "            self.topic_d += data\n",
    "\n",
    "    def start_reuters(self, attributes):\n",
    "        pass\n",
    "\n",
    "    def end_reuters(self):\n",
    "        self.body = re.sub(r\"\\s+\", r\" \", self.body)\n",
    "        self.docs.append(\n",
    "            {\"title\": self.title, \"body\": self.body, \"topics\": self.topics}\n",
    "        )\n",
    "        self._reset()\n",
    "\n",
    "    def start_title(self, attributes):\n",
    "        self.in_title = 1\n",
    "\n",
    "    def end_title(self):\n",
    "        self.in_title = 0\n",
    "\n",
    "    def start_body(self, attributes):\n",
    "        self.in_body = 1\n",
    "\n",
    "    def end_body(self):\n",
    "        self.in_body = 0\n",
    "\n",
    "    def start_topics(self, attributes):\n",
    "        self.in_topics = 1\n",
    "\n",
    "    def end_topics(self):\n",
    "        self.in_topics = 0\n",
    "\n",
    "    def start_d(self, attributes):\n",
    "        self.in_topic_d = 1\n",
    "\n",
    "    def end_d(self):\n",
    "        self.in_topic_d = 0\n",
    "        self.topics.append(self.topic_d)\n",
    "        self.topic_d = \"\"\n",
    "\n",
    "\n",
    "def stream_reuters_documents(data_path=None):\n",
    "\n",
    "    DOWNLOAD_URL = (\n",
    "        \"http://archive.ics.uci.edu/ml/machine-learning-databases/\"\n",
    "        \"reuters21578-mld/reuters21578.tar.gz\"\n",
    "    )\n",
    "    ARCHIVE_SHA256 = \"3bae43c9b14e387f76a61b6d82bf98a4fb5d3ef99ef7e7075ff2ccbcf59f9d30\"\n",
    "    ARCHIVE_FILENAME = \"reuters21578.tar.gz\"\n",
    "\n",
    "    if data_path is None:\n",
    "        data_path = Path(get_data_home()) / \"reuters\"\n",
    "    else:\n",
    "        data_path = Path(data_path)\n",
    "    if not data_path.exists():\n",
    "        \"\"\"Download the dataset.\"\"\"\n",
    "        print(\"downloading dataset (once and for all) into %s\" % data_path)\n",
    "        data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        def progress(blocknum, bs, size):\n",
    "            total_sz_mb = \"%.2f MB\" % (size / 1e6)\n",
    "            current_sz_mb = \"%.2f MB\" % ((blocknum * bs) / 1e6)\n",
    "            if _not_in_sphinx():\n",
    "                sys.stdout.write(\"\\rdownloaded %s / %s\" % (current_sz_mb, total_sz_mb))\n",
    "\n",
    "        archive_path = data_path / ARCHIVE_FILENAME\n",
    "\n",
    "        urlretrieve(DOWNLOAD_URL, filename=archive_path, reporthook=progress)\n",
    "        if _not_in_sphinx():\n",
    "            sys.stdout.write(\"\\r\")\n",
    "\n",
    "        assert sha256(archive_path.read_bytes()).hexdigest() == ARCHIVE_SHA256\n",
    "\n",
    "        print(\"untarring Reuters dataset...\")\n",
    "        tarfile.open(archive_path, \"r:gz\").extractall(data_path)\n",
    "        print(\"done.\")\n",
    "\n",
    "    parser = ReutersParser()\n",
    "    for filename in data_path.glob(\"*.sgm\"):\n",
    "        for doc in parser.parse(open(filename, \"rb\")):\n",
    "            yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading dataset (once and for all) into /root/scikit_learn_data/reuters\n"
     ]
    },
    {
     "ename": "IncompleteRead",
     "evalue": "IncompleteRead(4021 bytes read)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:591\u001b[0m, in \u001b[0;36mHTTPResponse._read_chunked\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 591\u001b[0m value\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_safe_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_left\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:632\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m<\u001b[39m amt:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(data, amt\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(data))\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[0;31mIncompleteRead\u001b[0m: IncompleteRead(546 bytes read, 3550 more expected)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m n_test_documents \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     55\u001b[0m tick \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 56\u001b[0m X_test_text, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mget_minibatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m parsing_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m tick\n\u001b[1;32m     58\u001b[0m tick \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[3], line 31\u001b[0m, in \u001b[0;36mget_minibatch\u001b[0;34m(doc_iter, size, pos_class)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_minibatch\u001b[39m(doc_iter, size, pos_class\u001b[38;5;241m=\u001b[39mpositive_class):\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Extract a minibatch of examples, return a tuple X_text, y.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    Note: size is before excluding invalid docs with no topics assigned.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     32\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{title}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{body}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdoc), pos_class \u001b[38;5;129;01min\u001b[39;00m doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopics\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mislice(doc_iter, size)\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopics\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     35\u001b[0m     ]\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray([], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m), np\u001b[38;5;241m.\u001b[39masarray([], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 31\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_minibatch\u001b[39m(doc_iter, size, pos_class\u001b[38;5;241m=\u001b[39mpositive_class):\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Extract a minibatch of examples, return a tuple X_text, y.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    Note: size is before excluding invalid docs with no topics assigned.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     32\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{title}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{body}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdoc), pos_class \u001b[38;5;129;01min\u001b[39;00m doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopics\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mislice(doc_iter, size)\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopics\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     35\u001b[0m     ]\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray([], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m), np\u001b[38;5;241m.\u001b[39masarray([], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 106\u001b[0m, in \u001b[0;36mstream_reuters_documents\u001b[0;34m(data_path)\u001b[0m\n\u001b[1;32m    102\u001b[0m         sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124mdownloaded \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (current_sz_mb, total_sz_mb))\n\u001b[1;32m    104\u001b[0m archive_path \u001b[38;5;241m=\u001b[39m data_path \u001b[38;5;241m/\u001b[39m ARCHIVE_FILENAME\n\u001b[0;32m--> 106\u001b[0m \u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDOWNLOAD_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marchive_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreporthook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _not_in_sphinx():\n\u001b[1;32m    108\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:270\u001b[0m, in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    267\u001b[0m     reporthook(blocknum, bs, size)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     block \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m block:\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:459\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked:\n\u001b[0;32m--> 459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:597\u001b[0m, in \u001b[0;36mHTTPResponse._read_chunked\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(value)\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m IncompleteRead:\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(value))\n",
      "\u001b[0;31mIncompleteRead\u001b[0m: IncompleteRead(4021 bytes read)"
     ]
    }
   ],
   "source": [
    "# 텍스트 문서를 벡터 형식으로 변환\n",
    "vectorizer = HashingVectorizer(\n",
    "    decode_error=\"ignore\", n_features=2**18, alternate_sign=False\n",
    ")\n",
    "\n",
    "data_stream = stream_reuters_documents()\n",
    "\n",
    "all_classes = np.array([0, 1])\n",
    "positive_class = \"acq\"\n",
    "\n",
    "# 분류기를 정의\n",
    "partial_fit_classifiers = {\n",
    "    \"SGD\": SGDClassifier(max_iter=5),\n",
    "    \"Perceptron\": Perceptron(),\n",
    "    \"NB Multinomial\": MultinomialNB(alpha=0.01),\n",
    "    \"Passive-Aggressive\": PassiveAggressiveClassifier(),\n",
    "}\n",
    "\n",
    "\n",
    "def get_minibatch(doc_iter, size, pos_class=positive_class):\n",
    "\n",
    "    data = [\n",
    "        (\"{title}\\n\\n{body}\".format(**doc), pos_class in doc[\"topics\"])\n",
    "        for doc in itertools.islice(doc_iter, size)\n",
    "        if doc[\"topics\"]\n",
    "    ]\n",
    "    if not len(data):\n",
    "        return np.asarray([], dtype=int), np.asarray([], dtype=int)\n",
    "    X_text, y = zip(*data)\n",
    "    return X_text, np.asarray(y, dtype=int)\n",
    "\n",
    "\n",
    "def iter_minibatches(doc_iter, minibatch_size):\n",
    "    X_text, y = get_minibatch(doc_iter, minibatch_size)\n",
    "    while len(X_text):\n",
    "        yield X_text, y\n",
    "        X_text, y = get_minibatch(doc_iter, minibatch_size)\n",
    "\n",
    "# 테스트 통계를 초기화\n",
    "test_stats = {\"n_test\": 0, \"n_test_pos\": 0}\n",
    "\n",
    "n_test_documents = 1000\n",
    "tick = time.time()\n",
    "X_test_text, y_test = get_minibatch(data_stream, 1000)\n",
    "parsing_time = time.time() - tick\n",
    "tick = time.time()\n",
    "X_test = vectorizer.transform(X_test_text)\n",
    "vectorizing_time = time.time() - tick\n",
    "test_stats[\"n_test\"] += len(y_test)\n",
    "test_stats[\"n_test_pos\"] += sum(y_test)\n",
    "print(\"Test set is %d documents (%d positive)\" % (len(y_test), sum(y_test)))\n",
    "\n",
    "\n",
    "def progress(cls_name, stats):\n",
    "    \"\"\"Report progress information, return a string.\"\"\"\n",
    "    duration = time.time() - stats[\"t0\"]\n",
    "    s = \"%20s classifier : \\t\" % cls_name\n",
    "    s += \"%(n_train)6d train docs (%(n_train_pos)6d positive) \" % stats\n",
    "    s += \"%(n_test)6d test docs (%(n_test_pos)6d positive) \" % test_stats\n",
    "    s += \"accuracy: %(accuracy).3f \" % stats\n",
    "    s += \"in %.2fs (%5d docs/s)\" % (duration, stats[\"n_train\"] / duration)\n",
    "    return s\n",
    "\n",
    "\n",
    "cls_stats = {}\n",
    "\n",
    "for cls_name in partial_fit_classifiers:\n",
    "    stats = {\n",
    "        \"n_train\": 0,\n",
    "        \"n_train_pos\": 0,\n",
    "        \"accuracy\": 0.0,\n",
    "        \"accuracy_history\": [(0, 0)],\n",
    "        \"t0\": time.time(),\n",
    "        \"runtime_history\": [(0, 0)],\n",
    "        \"total_fit_time\": 0.0,\n",
    "    }\n",
    "    cls_stats[cls_name] = stats\n",
    "\n",
    "get_minibatch(data_stream, n_test_documents)\n",
    "\n",
    "minibatch_size = 1000\n",
    "\n",
    "\n",
    "minibatch_iterators = iter_minibatches(data_stream, minibatch_size)\n",
    "total_vect_time = 0.0\n",
    "\n",
    "# 각 분류기에 대해 부분적인 학습을 수행\n",
    "for i, (X_train_text, y_train) in enumerate(minibatch_iterators):\n",
    "    tick = time.time()\n",
    "    X_train = vectorizer.transform(X_train_text)\n",
    "    total_vect_time += time.time() - tick\n",
    "\n",
    "    for cls_name, cls in partial_fit_classifiers.items():\n",
    "        tick = time.time()\n",
    "        cls.partial_fit(X_train, y_train, classes=all_classes)\n",
    "\n",
    "        cls_stats[cls_name][\"total_fit_time\"] += time.time() - tick\n",
    "        cls_stats[cls_name][\"n_train\"] += X_train.shape[0]\n",
    "        cls_stats[cls_name][\"n_train_pos\"] += sum(y_train)\n",
    "        tick = time.time()\n",
    "        cls_stats[cls_name][\"accuracy\"] = cls.score(X_test, y_test)\n",
    "        cls_stats[cls_name][\"prediction_time\"] = time.time() - tick\n",
    "        acc_history = (cls_stats[cls_name][\"accuracy\"], cls_stats[cls_name][\"n_train\"])\n",
    "        cls_stats[cls_name][\"accuracy_history\"].append(acc_history)\n",
    "        run_history = (\n",
    "            cls_stats[cls_name][\"accuracy\"],\n",
    "            total_vect_time + cls_stats[cls_name][\"total_fit_time\"],\n",
    "        )\n",
    "        cls_stats[cls_name][\"runtime_history\"].append(run_history)\n",
    "\n",
    "        if i % 3 == 0:\n",
    "            print(progress(cls_name, cls_stats[cls_name]))\n",
    "    if i % 3 == 0:\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
