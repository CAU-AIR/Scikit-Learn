{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic predictions with Gaussian process classification (GPC)\n",
    "- 하이퍼파라미터의 다양한 선택에 따른 RBF 커널의 GPC(Gaussian Process Classifier) 예측 확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "# 데이터 생성\n",
    "train_size = 50\n",
    "rng = np.random.RandomState(0)\n",
    "X = rng.uniform(0, 5, 100)[:, np.newaxis]\n",
    "y = np.array(X[:, 0] > 2.5, dtype=int)\n",
    "\n",
    "# 고정된 하이퍼파라미터와 최적화된 하이퍼파라미터를 가우스 프로세스를 지정\n",
    "gp_fix = GaussianProcessClassifier(kernel=1.0 * RBF(length_scale=1.0), optimizer=None)\n",
    "gp_fix.fit(X[:train_size], y[:train_size])\n",
    "\n",
    "gp_opt = GaussianProcessClassifier(kernel=1.0 * RBF(length_scale=1.0))\n",
    "gp_opt.fit(X[:train_size], y[:train_size])\n",
    "\n",
    "print(\n",
    "    \"Log Marginal Likelihood (initial): %.3f\"\n",
    "    % gp_fix.log_marginal_likelihood(gp_fix.kernel_.theta)\n",
    ")\n",
    "print(\n",
    "    \"Log Marginal Likelihood (optimized): %.3f\"\n",
    "    % gp_opt.log_marginal_likelihood(gp_opt.kernel_.theta)\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Accuracy: %.3f (initial) %.3f (optimized)\"\n",
    "    % (\n",
    "        accuracy_score(y[:train_size], gp_fix.predict(X[:train_size])),\n",
    "        accuracy_score(y[:train_size], gp_opt.predict(X[:train_size])),\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Log-loss: %.3f (initial) %.3f (optimized)\"\n",
    "    % (\n",
    "        log_loss(y[:train_size], gp_fix.predict_proba(X[:train_size])[:, 1]),\n",
    "        log_loss(y[:train_size], gp_opt.predict_proba(X[:train_size])[:, 1]),\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# posteriors 그리기\n",
    "plt.figure()\n",
    "plt.scatter(\n",
    "    X[:train_size, 0], y[:train_size], c=\"k\", label=\"Train data\", edgecolors=(0, 0, 0)\n",
    ")\n",
    "plt.scatter(\n",
    "    X[train_size:, 0], y[train_size:], c=\"g\", label=\"Test data\", edgecolors=(0, 0, 0)\n",
    ")\n",
    "X_ = np.linspace(0, 5, 100)\n",
    "plt.plot(\n",
    "    X_,\n",
    "    gp_fix.predict_proba(X_[:, np.newaxis])[:, 1],\n",
    "    \"r\",\n",
    "    label=\"Initial kernel: %s\" % gp_fix.kernel_,\n",
    ")\n",
    "plt.plot(\n",
    "    X_,\n",
    "    gp_opt.predict_proba(X_[:, np.newaxis])[:, 1],\n",
    "    \"b\",\n",
    "    label=\"Optimized kernel: %s\" % gp_opt.kernel_,\n",
    ")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Class 1 probability\")\n",
    "plt.xlim(0, 5)\n",
    "plt.ylim(-0.25, 1.5)\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "# LML landscape 그리기\n",
    "plt.figure()\n",
    "theta0 = np.logspace(0, 8, 30)\n",
    "theta1 = np.logspace(-1, 1, 29)\n",
    "Theta0, Theta1 = np.meshgrid(theta0, theta1)\n",
    "LML = [\n",
    "    [\n",
    "        gp_opt.log_marginal_likelihood(np.log([Theta0[i, j], Theta1[i, j]]))\n",
    "        for i in range(Theta0.shape[0])\n",
    "    ]\n",
    "    for j in range(Theta0.shape[1])\n",
    "]\n",
    "LML = np.array(LML).T\n",
    "plt.plot(\n",
    "    np.exp(gp_fix.kernel_.theta)[0], np.exp(gp_fix.kernel_.theta)[1], \"ko\", zorder=10\n",
    ")\n",
    "plt.plot(\n",
    "    np.exp(gp_opt.kernel_.theta)[0], np.exp(gp_opt.kernel_.theta)[1], \"ko\", zorder=10\n",
    ")\n",
    "plt.pcolor(Theta0, Theta1, LML)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Magnitude\")\n",
    "plt.ylabel(\"Length-scale\")\n",
    "plt.title(\"Log-marginal-likelihood\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
