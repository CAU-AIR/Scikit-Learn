{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "n_samples, n_features = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of physical cores: 8\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "N_CORES = joblib.cpu_count(only_physical_cores=True)\n",
    "print(f\"Number of physical cores: {N_CORES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestRegressor(\n",
    "        min_samples_leaf=5, random_state=0, n_jobs=N_CORES\n",
    "    ),\n",
    "    \"Hist Gradient Boosting\": HistGradientBoostingRegressor(\n",
    "        max_leaf_nodes=15, random_state=0, early_stopping=False\n",
    "    ),\n",
    "}\n",
    "param_grids = {\n",
    "    \"Random Forest\": {\"n_estimators\": [10, 20, 50, 100]},\n",
    "    \"Hist Gradient Boosting\": {\"max_iter\": [10, 20, 50, 100, 300, 500]},\n",
    "}\n",
    "cv = KFold(n_splits=4, shuffle=True, random_state=0)\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grids[name],\n",
    "        return_train_score=True,\n",
    "        cv=cv,\n",
    "    ).fit(X, y)\n",
    "    result = {\"model\": name, \"cv_results\": pd.DataFrame(grid_search.cv_results_)}\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly.express'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcolors\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msubplots\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_subplots\n\u001b[1;32m      5\u001b[0m fig \u001b[38;5;241m=\u001b[39m make_subplots(\n\u001b[1;32m      6\u001b[0m     rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      7\u001b[0m     cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m      8\u001b[0m     shared_yaxes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     subplot_titles\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain time vs score\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredict time vs score\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly.express'"
     ]
    }
   ],
   "source": [
    "import plotly.colors as colors\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    shared_yaxes=True,\n",
    "    subplot_titles=[\"Train time vs score\", \"Predict time vs score\"],\n",
    ")\n",
    "model_names = [result[\"model\"] for result in results]\n",
    "colors_list = colors.qualitative.Plotly * (\n",
    "    len(model_names) // len(colors.qualitative.Plotly) + 1\n",
    ")\n",
    "\n",
    "for idx, result in enumerate(results):\n",
    "    cv_results = result[\"cv_results\"].round(3)\n",
    "    model_name = result[\"model\"]\n",
    "    param_name = list(param_grids[model_name].keys())[0]\n",
    "    cv_results[param_name] = cv_results[\"param_\" + param_name]\n",
    "    cv_results[\"model\"] = model_name\n",
    "\n",
    "    scatter_fig = px.scatter(\n",
    "        cv_results,\n",
    "        x=\"mean_fit_time\",\n",
    "        y=\"mean_test_score\",\n",
    "        error_x=\"std_fit_time\",\n",
    "        error_y=\"std_test_score\",\n",
    "        hover_data=param_name,\n",
    "        color=\"model\",\n",
    "    )\n",
    "    line_fig = px.line(\n",
    "        cv_results,\n",
    "        x=\"mean_fit_time\",\n",
    "        y=\"mean_test_score\",\n",
    "    )\n",
    "\n",
    "    scatter_trace = scatter_fig[\"data\"][0]\n",
    "    line_trace = line_fig[\"data\"][0]\n",
    "    scatter_trace.update(marker=dict(color=colors_list[idx]))\n",
    "    line_trace.update(line=dict(color=colors_list[idx]))\n",
    "    fig.add_trace(scatter_trace, row=1, col=1)\n",
    "    fig.add_trace(line_trace, row=1, col=1)\n",
    "\n",
    "    scatter_fig = px.scatter(\n",
    "        cv_results,\n",
    "        x=\"mean_score_time\",\n",
    "        y=\"mean_test_score\",\n",
    "        error_x=\"std_score_time\",\n",
    "        error_y=\"std_test_score\",\n",
    "        hover_data=param_name,\n",
    "    )\n",
    "    line_fig = px.line(\n",
    "        cv_results,\n",
    "        x=\"mean_score_time\",\n",
    "        y=\"mean_test_score\",\n",
    "    )\n",
    "\n",
    "    scatter_trace = scatter_fig[\"data\"][0]\n",
    "    line_trace = line_fig[\"data\"][0]\n",
    "    scatter_trace.update(marker=dict(color=colors_list[idx]))\n",
    "    line_trace.update(line=dict(color=colors_list[idx]))\n",
    "    fig.add_trace(scatter_trace, row=1, col=2)\n",
    "    fig.add_trace(line_trace, row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis=dict(title=\"Train time (s) - lower is better\"),\n",
    "    yaxis=dict(title=\"Test R2 score - higher is better\"),\n",
    "    xaxis2=dict(title=\"Predict time (s) - lower is better\"),\n",
    "    legend=dict(x=0.72, y=0.05, traceorder=\"normal\", borderwidth=1),\n",
    "    title=dict(x=0.5, text=\"Speed-score trade-off of tree-based ensembles\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
