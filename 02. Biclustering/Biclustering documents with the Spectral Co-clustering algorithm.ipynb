{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biclustering documents with the Spectral Co-clustering algorithm\n",
    "- 20개의 뉴스 그룹 데이터 셋에 대한 Spectrum Co-clustering 알고리즘을 보인다.\n",
    "    - `comp.os.ms-window.misc` 카테고리는 데이터만 포함된 게시물이 많기 때문에 제외된다.\n",
    "- TF-IDF 벡터화된 게시물은 단어 빈도 행렬을 형성한 다음 Spectrum Co-clustering 알고리즘을 사용하여 바이클러스터링 된다.\n",
    "    - 결과적인 문서-단어 바이클러스터는 해당 하위 집합 문서에서 더 자주 사용되는 하위 집합 단어를 나타낸다.\n",
    "- 베스트 바이클러스터의 경우, 가장 일반적인 문서 카테고리와 10개의 가장 중요한 단어가 프린트된다.\n",
    "    - 베스트 바이클러스터는 정규화된 cut에 의해 결정된다.\n",
    "    - 중요한 단어는 바이클러스터 내부와 외부의 합계를 비교하여 결정된다.\n",
    "- 비교를 위해 `MiniBatchKmeans`를 사용하여 문서를 클러스터링 한다.\n",
    "    - 바이클러스터에서 파생된 문서 클러스터는 `MiniBatchKmeans`에서 찾은 클러스터보다 더 나은 V-measure를 달성한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Biclustering(양방향 클러스터링): 데이터를 동시에 행과 열 양쪽에서 클러스터로 그룹화하는 기술\n",
    "    - 유전자 발현 데이터\n",
    "    - 이미지 분석\n",
    "    - 텍스트 마이닝\n",
    "- Spectral Co-clustering algorithm: 바이클러스터링 알고리즘 중 하나, 행렬의 구조적 패턴을 파악하고자 할 때 사용\n",
    "    - 데이터를 특잇값 분해하여 잠재적인 패턴 추출\n",
    "    - 추출된 특잇값을 기반으로 그래프 생성\n",
    "    - 그래프 분할 알고리즘을 사용하여 행과 열을 클러스터로 그룹화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from collections import defaultdict\n",
    "from time import time\n",
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans, SpectralCoclustering\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.cluster import v_measure_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰 리스트를 받아와서 해당 리스트의 각 토큰이 숫자로 시작하면 그 토큰을 `#NUMBER`로 대체하는 함수인 `number_normalizer`를 정의한다.\n",
    "- 텍스트 데이터를 처리할 때, 숫자로 시작하는 단어들은 불필요한 노이즈로 작용할 수 있다.\n",
    "- 숫자로 시작하는 토큰들을 `#NUMBER`로 대체하여 차원 축소의 효과를 낸다.\n",
    "    - ['123abc', 'apple', '45xyz'] $->$ ['#NUMBER', 'apple', '#NUMBER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_normalizer(tokens):\n",
    "    \"\"\"\n",
    "    모든 숫자 토큰을 placeholder에 매핑한다.\n",
    "    많은 application에서 숫자로 시작하는 토근은 직접적으로 유용하지는 않지만 토큰이 존재한다는 사실이 관련될 수는 있다.\n",
    "    이러한 형태의 차원 감소를 적용함으로 일부 방법은 더 나은 성능을 발휘할 수 있다.\n",
    "    \"\"\"\n",
    "    return (\"#NUMBER\" if token[0].isdigit() else token for token in tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20개의 다른 주제를 가진 뉴스 그룹 데이터를 처리하고, 특정 벡터화 및 클러스터링 기법을 적용하여 문서를 그룹화하는 과정\n",
    "- `NumberNormalizingVectorizer`\n",
    "    - `TfidfVectorizer`를 상속받아 만들어진다.\n",
    "    - `build_tokenizer` 메서드를 오버라이드하여, 기존의 토크나이저에 추가적인 숫자 정규화 기능을 포함한다.\n",
    "- 데이터 로딩 및 전처리:\n",
    "    - 뉴스 그룹 데이터를 `fetch_20newsgroups`함수를 사용해 불러온다.\n",
    "    - `NumberNormalizingVectorizer`를 사용하여 TF-IDF 벡터화를 수행한다.\n",
    "        - stop words를 제거하고, 최소 문서 빈도(`min_df`)가 5인 단어만 사용한다.\n",
    "- Co-clustering, Kmeans clustering\n",
    "    - `SpectralCoclustering`과 `MiniBatchKMeans`를 사용하여 각각 코 클러스터링과 K-평균 클러스터링을 수행한다.\n",
    "- 성능 평가\n",
    "    - `v_measure_score`함수를 사용해 클러스터링 결과의 성능을 측정한다.\n",
    "        - V-measure는 정밀도(Precision)와 재현율(Recall)의 조화 평균으로 계산되는 클러스터링 지표이다. \n",
    "- 출력\n",
    "    - 클러스터링 과정과 결과의 V-measure 스코어가 출력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing...\n",
      "Coclustering...\n",
      "Done in 0.54s. V-measure: 0.4487\n",
      "MiniBatchKmeans...\n",
      "Done in 1.95s. V-measure: 0.2690\n"
     ]
    }
   ],
   "source": [
    "class NumberNormalizingVectorizer(TfidfVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenize = super().build_tokenizer()\n",
    "        return lambda doc: list(number_normalizer(tokenize(doc)))\n",
    "    \n",
    "# comp.os.ms-windows.misc 제외\n",
    "categories = [\n",
    "    \"alt.atheism\",\n",
    "    \"comp.graphics\",\n",
    "    \"comp.sys.ibm.pc.hardware\",\n",
    "    \"comp.sys.mac.hardware\",\n",
    "    \"comp.windows.x\",\n",
    "    \"misc.forsale\",\n",
    "    \"rec.autos\",\n",
    "    \"rec.motorcycles\",\n",
    "    \"rec.sport.baseball\",\n",
    "    \"rec.sport.hockey\",\n",
    "    \"sci.crypt\",\n",
    "    \"sci.electronics\",\n",
    "    \"sci.med\",\n",
    "    \"sci.space\",\n",
    "    \"soc.religion.christian\",\n",
    "    \"talk.politics.mideast\",\n",
    "    \"talk.politics.misc\",\n",
    "    \"talk.religion.misc\",\n",
    "]\n",
    "newsgroups = fetch_20newsgroups(categories=categories)\n",
    "y_true = newsgroups.target\n",
    "\n",
    "vectorizer = NumberNormalizingVectorizer(stop_words=\"english\", min_df=5)\n",
    "cocluster = SpectralCoclustering(\n",
    "    n_clusters=len(categories), svd_method=\"arpack\", random_state=0\n",
    ")\n",
    "kmeans = MiniBatchKMeans(\n",
    "    n_clusters=len(categories), batch_size=20000, random_state=0, n_init=3\n",
    ")\n",
    "\n",
    "print(\"Vectorizing...\")\n",
    "X = vectorizer.fit_transform(newsgroups.data)\n",
    "\n",
    "print(\"Coclustering...\")\n",
    "start_time = time()\n",
    "cocluster.fit(X)\n",
    "y_cocluster = cocluster.row_labels_\n",
    "print(\n",
    "    \"Done in {:.2f}s. V-measure: {:.4f}\".format(time() - start_time, v_measure_score(y_cocluster, y_true))\n",
    ")\n",
    "\n",
    "print(\"MiniBatchKmeans...\")\n",
    "start_time = time()\n",
    "y_kemans = kmeans.fit_predict(X)\n",
    "print(\n",
    "     \"Done in {:.2f}s. V-measure: {:.4f}\".format(time() - start_time, v_measure_score(y_kemans, y_true))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-clustering의 결과에서 바이클러스터에 대한 Normalized Cut(Ncut)을 꼐산하는 함수 정의\n",
    "- Normalized Cut은 클러스터 내의 가중치를 클러스터 외의 가중치로 나눈 값으로 클러스터링의 품질을 측정하는 지표 중 하나이다.\n",
    "- `bicluster_ncut(i)`: `i`는 바이클러스터의 인덱스를 나타낸다.\n",
    "    - `cocluster.get_indices(i)`: 바이클러스터 `i`의 행과 열 인덱스를 가져온다.\n",
    "    - `np.logical_not(cocluster.rows_[i])`: 행과 열에 대한 complement 인덱스(주어진 집합에 속하지 않는 모든 원소들로 이루어진 집합)를 계산한다.\n",
    "    - `X[rows][:, cols].sum()`: 바이클러스터 내의 가중치 합을 계산한다.\n",
    "    - `X[row_complement][:, cols].sum() + X[rows][:, col_complement].sum()`: 바이클러스터 외의 가중치 합을 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "document_names = list(newsgroups.target_names[i] for i in newsgroups.target)\n",
    "\n",
    "def bicluster_ncut(i):\n",
    "    rows, cols = cocluster.get_indices(i)\n",
    "    if not (np.any(rows) and np.any(cols)):\n",
    "        import sys\n",
    "\n",
    "        return sys.float_info.max\n",
    "    row_complement = np.nonzero(np.logical_not(cocluster.rows_[i]))[0]\n",
    "    col_complement = np.nonzero(np.logical_not(cocluster.columns_[i]))[0]\n",
    "    # X[rows[:, np.newaxis], cols].sum()이 scipy <= 0.16  \n",
    "    weight = X[rows][:, cols].sum()\n",
    "    cut = X[row_complement][:, cols].sum() + X[rows][:, col_complement].sum()\n",
    "    return cut / weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 바이클러스터의 품질을 측정한 결과를 분석하고, 가장 좋은 바이클러스터를 선택해 해당 바이클러스터에 속하는 문서와 단어들, 카테고리 및 중요한 단어들을 출력\n",
    "- `most_common(d)` 함수\n",
    "    - `defaultdict(int)`형태의 딕셔너리를 받아 해당 딕셔너리를 내림차순으로 정렬하여 반환한다.\n",
    "    - 빈도수가 높은 항목을 찾을 때 사용된다.\n",
    "- `bicluster_ncuts` 계산\n",
    "    - 각 바이클러스터에 대한 Normalized Cut 값을 계산하여 리스트에 저장한다.\n",
    "- 가장 좋은 바이클러스터 선택:\n",
    "    - `np.argsort(bicluster_ncuts)[:5]`을 사용해 Normalized Cut 값이 잔은 상위 5개의 바이클러스터를 선택한다.\n",
    "- 선택된 바이클러스터에 대한 분석 및 출력:\n",
    "    - 선택된 바이클러스터에 대해 해당하는 문서와 단어들을 가져와서 분석한다.\n",
    "    - 바이클러스터에 속한 문서의 카테고리 분포를 계산하고 출력한다.\n",
    "    - 바이클러스터에 속한 단어들 중에서 중요한 단어들을 계산하고 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best biclusters:\n",
      "----------------\n",
      "bicluster 0 : 8 documents, 6 words\n",
      "categories   : 100% talk.politics.mideast\n",
      "words        : cosmo, angmar, alfalfa, alphalpha, proline, benson\n",
      "\n",
      "bicluster 1 : 1092 documents, 2929 words\n",
      "categories   : 30% talk.politics.mideast, 27% soc.religion.christian, 26% alt.atheism\n",
      "words        : god, jesus, christians, atheists, morality, kent, sin, belief, objective, resurrection\n",
      "\n",
      "bicluster 2 : 2225 documents, 2862 words\n",
      "categories   : 18% comp.sys.mac.hardware, 16% comp.sys.ibm.pc.hardware, 16% sci.electronics\n",
      "words        : voltage, shipping, receiver, circuit, compression, digital, processing, scope, baalke, package\n",
      "\n",
      "bicluster 3 : 1505 documents, 3837 words\n",
      "categories   : 24% talk.politics.misc, 18% sci.med, 17% soc.religion.christian\n",
      "words        : geb, banks, gordon, drugs, kaldis, dyer, br, surrender, noring, n3jxp\n",
      "\n",
      "bicluster 4 : 1722 documents, 2666 words\n",
      "categories   : 27% rec.motorcycles, 23% rec.autos, 13% misc.forsale\n",
      "words        : bike, car, dod, ride, motorcycle, engine, bikes, bmw, helmet, cars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def most_common(d):\n",
    "    \"\"\"\n",
    "    가장 높은 값을 가진 defaultdict(int)의 항목이다.\n",
    "    Python >= 2.7에서 Counter.most_common과 비슷하다.\n",
    "    \"\"\"    \n",
    "    return sorted(d.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "bicluster_ncuts = list(bicluster_ncut(i) for i in range(len(newsgroups.target_names)))\n",
    "best_idx = np.argsort(bicluster_ncuts)[:5]\n",
    "\n",
    "print()\n",
    "print(\"Best biclusters:\")\n",
    "print(\"----------------\")\n",
    "for idx, cluster in enumerate(best_idx):\n",
    "    n_rows, n_cols = cocluster.get_shape(cluster)\n",
    "    cluster_docs, cluster_words = cocluster.get_indices(cluster)\n",
    "    if not len(cluster_docs) or not len(cluster_words):\n",
    "        continue\n",
    "\n",
    "    # categories\n",
    "    counter = defaultdict(int)\n",
    "    for i in cluster_docs:\n",
    "        counter[document_names[i]] += 1\n",
    "    cat_string = \", \".join(\n",
    "        \"{:.0f}% {}\".format(float(c) / n_rows * 100, name)\n",
    "        for name, c in most_common(counter)[:3]\n",
    "    )\n",
    "\n",
    "    # words\n",
    "    out_of_cluster_docs = cocluster.row_labels_ != cluster\n",
    "    out_of_cluster_docs = np.where(out_of_cluster_docs)[0]\n",
    "    word_col = X[:, cluster_words]\n",
    "    word_scores = np.array(\n",
    "        word_col[cluster_docs, :].sum(axis=0)\n",
    "        - word_col[out_of_cluster_docs, :].sum(axis=0)\n",
    "    )\n",
    "    word_scores = word_scores.ravel()\n",
    "    important_words = list(\n",
    "        feature_names[cluster_words[i]] for i in word_scores.argsort()[:-11:-1]\n",
    "    )\n",
    "\n",
    "    print(\"bicluster {} : {} documents, {} words\".format(idx, n_rows, n_cols))\n",
    "    print(\"categories   : {}\".format(cat_string))\n",
    "    print(\"words        : {}\\n\".format(\", \".join(important_words)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
